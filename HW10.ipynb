{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaF0p36BNR6HhicsM0P2y/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Emmnmm/sta365hw10/blob/main/HW10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1\n",
        "define : <br>\n",
        "- Gaussian Processes (GPs) and stochastic processes generally <br>\n",
        "- Variational inference using the Evidence Lower Bound (ELBO) <br>"
      ],
      "metadata": {
        "id": "qFw0Bg2d-c-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Stochastic process is a collection of random variables indexed by time or space, representing a system that evolves under uncertainty. Stochastic processes are widely used as mathematical models for systems and phenomena that appear to vary randomly, such as stock prices, weather patterns, and population growth. <br>\n",
        "The Gaussian Processes (GPs) is a stochastic process where every finite collection of those random variables will follow a multivariate normal distribution, which makes GPs very flexible for modeling complex, non-linear relationships. The key properties of GPs are that it is defined by a mean function and a covariance (kernel) function. The kernel function determines how points influence each other, allowing GPs to model smooth, periodic, or non-stationary functions. <br>\n",
        "We use Variational Inference (VI) because, in Bayesian inference, the posterior distribution is often too complex to compute directly. As our marginal likelihood p(x) often has no closed-form solution or\n",
        "requires integrating over all possible parameter values which is impractical in high-dimensional spaces.<br>\n",
        "Variational inference is a method that uses optimization to find a simpler distribution that is close to the desired posterior to approximate it. The simpler distribution which is also defined as q(θ), is usually found by minimizing its KL divergence which can also be interpreted as minimizing the relative entropy between the two distributions. However, the KL divergence is hard to compute as it requires evaluating the true posterior distribution. This is why the alternative method of maximizing the ELBO is preferred. As the Evidence Lower Bound (ELBO), is a more tractable objective function that indirectly minimizes KL divergence, and maximizing it allows for a good posterior approximation while avoiding computing p(X).<br>"
      ],
      "metadata": {
        "id": "r5FrXd1LDskP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2"
      ],
      "metadata": {
        "id": "VqiQwWaRSRIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymc as pm\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "df = sns.load_dataset(\"mpg\").dropna()  # Drop missing values\n",
        "\n",
        "# Define features\n",
        "X = df[\"horsepower\"].values.reshape(-1, 1)  # Horsepower as input\n",
        "y = df[\"mpg\"].values  # MPG as target\n",
        "\n",
        "# Standardize both X and y\n",
        "X = (X - np.mean(X)) / np.std(X)\n",
        "y = (y - np.mean(y)) / np.std(y)\n",
        "\n",
        "# Define Gaussian Process Model\n",
        "with pm.Model() as gp_model:\n",
        "    mean_func = pm.gp.mean.Constant(c=0)  # Constant mean function\n",
        "    kernel = pm.gp.cov.ExpQuad(1, ls=1.0)  # Exponential Quadratic Kernel\n",
        "\n",
        "    gp = pm.gp.Marginal(mean_func=mean_func, cov_func=kernel)\n",
        "    sigma = pm.HalfNormal(\"sigma\", 1.0)  # Observation noise\n",
        "\n",
        "    y_obs = gp.marginal_likelihood(\"y_obs\", X=X, y=y, noise=sigma)\n",
        "\n",
        "    trace = pm.sample(1000, return_inferencedata=True, target_accept=0.9)\n",
        "\n",
        "# Make predictions over new horsepower values\n",
        "X_new = np.linspace(X.min(), X.max(), 200)[:, None]\n",
        "with gp_model:\n",
        "    mu, var = gp.predict(X_new, point=trace.posterior.mean(dim=[\"chain\", \"draw\"]), diag=True)\n",
        "    sd = np.sqrt(var)\n",
        "\n",
        "# Plot results on standardized scale\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(X, y, label=\"Observed MPG\", alpha=0.6, color=\"blue\")  # Original observations\n",
        "plt.plot(X_new, mu, label=\"GP Mean Estimate\", color=\"red\")  # GP predictions\n",
        "plt.fill_between(X_new.flatten(), mu - 2*sd, mu + 2*sd, alpha=0.3, color=\"red\", label=\"±2 Std. Dev.\")\n",
        "plt.title(\"Gaussian Process Regression on Fuel Efficiency vs Horsepower (Standardized)\")\n",
        "plt.xlabel(\"Horsepower (Standardized)\")\n",
        "plt.ylabel(\"MPG (Standardized)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Zua3A1d4ktQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Using the MPG dataset from seaborn with X as horsepower and Y as miles per gallon (mpg) <br>\n",
        "- predicting the MPG using the Gaussian Process model for a given horsepower value <br>"
      ],
      "metadata": {
        "id": "opmg0Hh2k5Lo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3"
      ],
      "metadata": {
        "id": "_TwXLUxXliNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import arviz as az\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pymc as pm\n",
        "import pytensor\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import scale"
      ],
      "metadata": {
        "id": "8eV87huppN69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "floatX = pytensor.config.floatX\n",
        "RANDOM_SEED = 9927\n",
        "rng = np.random.default_rng(RANDOM_SEED)\n",
        "az.style.use(\"arviz-darkgrid\")"
      ],
      "metadata": {
        "id": "Laq2ULjApXQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.pymc.io/projects/examples/en/latest/variational_inference/bayesian_neural_network_advi.html#model-specification\n",
        "\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target  # Add the target variable\n",
        "\n",
        "# Convert to binary classification (Setosa vs non-Setosa)\n",
        "df['binary_target'] = (df['target'] == 0).astype(int)  # Setosa = 1, others = 0\n",
        "\n",
        "# Inspect the first few rows\n",
        "print(df.head())\n",
        "\n",
        "# Define X (features) and Y (binary target)\n",
        "X = df[data.feature_names].values.astype(floatX)  # Features (all columns except 'target')\n",
        "Y = df['binary_target'].values.astype(floatX)   # Binary target (0 or 1)\n",
        "\n",
        "\n",
        "# Standardize the features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and test sets.\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X_scaled, Y, test_size=0.5, random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Test set shape:\", X_test.shape)"
      ],
      "metadata": {
        "id": "ayP4QYT4n0Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_nn(batch_size=50):\n",
        "    n_hidden = 5\n",
        "\n",
        "    # Initialize random weights between each layer\n",
        "    init_1 = rng.standard_normal(size=(X_train.shape[1], n_hidden)).astype(floatX)\n",
        "    init_2 = rng.standard_normal(size=(n_hidden, n_hidden)).astype(floatX)\n",
        "    init_out = rng.standard_normal(size=n_hidden).astype(floatX)\n",
        "\n",
        "    coords = {\n",
        "        \"hidden_layer_1\": np.arange(n_hidden),\n",
        "        \"hidden_layer_2\": np.arange(n_hidden),\n",
        "        \"train_cols\": np.arange(X_train.shape[1]),\n",
        "        \"obs_id\": np.arange(X_train.shape[0]),\n",
        "    }\n",
        "\n",
        "    with pm.Model(coords=coords) as neural_network:\n",
        "\n",
        "        # Define data variables using minibatches\n",
        "        X_data = pm.Data(\"X_data\", X_train, dims=(\"obs_id\", \"train_cols\"))\n",
        "        Y_data = pm.Data(\"Y_data\", Y_train, dims=\"obs_id\")\n",
        "\n",
        "        # Define minibatch variables\n",
        "        ann_input, ann_output = pm.Minibatch(X_data, Y_data, batch_size=batch_size)\n",
        "\n",
        "        # Weights from input to hidden layer\n",
        "        weights_in_1 = pm.Normal(\n",
        "            \"w_in_1\", 0, sigma=1, initval=init_1, dims=(\"train_cols\", \"hidden_layer_1\")\n",
        "        )\n",
        "\n",
        "        # Weights from 1st to 2nd layer\n",
        "        weights_1_2 = pm.Normal(\n",
        "            \"w_1_2\", 0, sigma=1, initval=init_2, dims=(\"hidden_layer_1\", \"hidden_layer_2\")\n",
        "        )\n",
        "\n",
        "        # Weights from hidden layer to output\n",
        "        weights_2_out = pm.Normal(\"w_2_out\", 0, sigma=1, initval=init_out, dims=\"hidden_layer_2\")\n",
        "\n",
        "        # Build neural-network using tanh activation function\n",
        "        act_1 = pm.math.tanh(pm.math.dot(ann_input, weights_in_1))\n",
        "        act_2 = pm.math.tanh(pm.math.dot(act_1, weights_1_2))\n",
        "        act_out = pm.math.sigmoid(pm.math.dot(act_2, weights_2_out))\n",
        "\n",
        "        # Binary classification -> Bernoulli likelihood\n",
        "        out = pm.Bernoulli(\n",
        "            \"out\",\n",
        "            act_out,\n",
        "            observed=ann_output,\n",
        "            total_size=X_train.shape[0],  # IMPORTANT for minibatches\n",
        "        )\n",
        "    return neural_network\n",
        "\n",
        "\n",
        "# Create the neural network model\n",
        "neural_network = construct_nn()"
      ],
      "metadata": {
        "id": "AWM-zK3yqBkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with neural_network:\n",
        "    approx = pm.fit(n=30_000)"
      ],
      "metadata": {
        "id": "uLoLPlcstT2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(approx.hist, alpha=0.3)\n",
        "plt.ylabel(\"ELBO\")\n",
        "plt.xlabel(\"iteration\");"
      ],
      "metadata": {
        "id": "16iINOPwt1LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trace = approx.sample(draws=5000)"
      ],
      "metadata": {
        "id": "eZ9piu2Yq8qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_posterior_predictive(X_test, Y_test, trace, n_hidden=5):\n",
        "    coords = {\n",
        "        \"hidden_layer_1\": np.arange(n_hidden),\n",
        "        \"hidden_layer_2\": np.arange(n_hidden),\n",
        "        \"train_cols\": np.arange(X_test.shape[1]),\n",
        "        \"obs_id\": np.arange(X_test.shape[0]),\n",
        "    }\n",
        "    with pm.Model(coords=coords):\n",
        "\n",
        "        ann_input = X_test\n",
        "        ann_output = Y_test\n",
        "\n",
        "        weights_in_1 = pm.Flat(\"w_in_1\", dims=(\"train_cols\", \"hidden_layer_1\"))\n",
        "        weights_1_2 = pm.Flat(\"w_1_2\", dims=(\"hidden_layer_1\", \"hidden_layer_2\"))\n",
        "        weights_2_out = pm.Flat(\"w_2_out\", dims=\"hidden_layer_2\")\n",
        "\n",
        "        # Build neural-network using tanh activation function\n",
        "        act_1 = pm.math.tanh(pm.math.dot(ann_input, weights_in_1))\n",
        "        act_2 = pm.math.tanh(pm.math.dot(act_1, weights_1_2))\n",
        "        act_out = pm.math.sigmoid(pm.math.dot(act_2, weights_2_out))\n",
        "\n",
        "        # Binary classification -> Bernoulli likelihood\n",
        "        out = pm.Bernoulli(\"out\", act_out, observed=ann_output)\n",
        "        return pm.sample_posterior_predictive(trace)\n",
        "\n",
        "\n",
        "ppc = sample_posterior_predictive(X_test, Y_test, trace)"
      ],
      "metadata": {
        "id": "pFdLdJcHqm7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = ppc.posterior_predictive[\"out\"].mean((\"chain\", \"draw\")) > 0.5"
      ],
      "metadata": {
        "id": "k5OZYGpwrAU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X_test[pred == 0, 0], X_test[pred == 0, 1], color=\"C0\", label=\"Predicted 0\")\n",
        "ax.scatter(X_test[pred == 1, 0], X_test[pred == 1, 1], color=\"C1\", label=\"Predicted 1\")\n",
        "sns.despine()\n",
        "ax.legend()\n",
        "ax.set(title=\"Predicted labels in testing set\", xlabel=\"X1\", ylabel=\"X2\");"
      ],
      "metadata": {
        "id": "eXqHL9iUrFjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy = {(Y_test == pred.values).mean() * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "MWTz-7oNtgKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Used the Iris dataset\n",
        "- Features (X): Sepal length, sepal width, petal length, petal width.\n",
        "- Target (Y): Binary target (Setosa vs. non-Setosa).\n",
        "\n"
      ],
      "metadata": {
        "id": "T65JVemquyMk"
      }
    }
  ]
}